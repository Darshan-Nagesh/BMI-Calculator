import numpy as np
import scipy.spatial
from collections import Counter
from sklearn.model_selection import LeaveOneOut
from sklearn import datasets
from sklearn.model_selection import train_test_split
from scipy.spatial.distance import cityblock
from sklearn.metrics import accuracy_score
import pandas as pd

#importing data from glass.csv
glass=pd.read_csv("glass.csv")
glass_x=glass[["RI","Na","Mg","Al","Si","K","Ca","Ba","Fe","Type"]].values
glass_y=glass["Type"].values

fruit=pd.read_csv("fruit.csv")
fruit_x=fruit[["fruit_label","fruit_name","fruit_subtype","mass","width","height","color_score"]].values
fruit_y=fruit["fruit_label"].values

class KNN:
    def __init__(self, k):
        self.k = k
        
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
        
    def distance(self, X1, X2):
        distance = scipy.spatial.distance.euclidean(X1, X2)
       
    
    def predict(self, X_test):
        final_output = []
        for i in range(len(X_test)):
            d = []
            votes = []
            for j in range(len(X_train)):
                dist = scipy.spatial.distance.euclidean(X_train[j] , X_test[i])
                d.append([dist, j])
            d.sort()
            d = d[0:self.k]
            for d, j in d:
                votes.append(y_train[j])
            ans = Counter(votes).most_common(1)[0][0]
            final_output.append(ans)
            
        return final_output
    
    def score(self, X_test, y_test):
        predictions = self.predict(X_test)
        return (predictions == y_test).sum() / len(y_test)

kvalues=[3,5,7]
for k in kvalues:
    print("###########################################################################")
    print(f"\nk-value= {k}")
    #splitting the data in 90-10
    print("\n90-10")
    X_train, X_test, y_train, y_test = train_test_split(glass_x, glass_y, random_state = 42, test_size = 0.1)
    clf = KNN(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    for i in prediction:
        print(i, end= ' ')

    prediction = y_test
    print("\n")
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

    #splitting the data to 70-30
    print("\n70-30")
    print("\n ")
    X_train, X_test, y_train, y_test = train_test_split(glass_x, glass_y, random_state = 42, test_size = 0.3)
    clf = KNN(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
 
    for i in prediction:
        print(i, end= ' ')

    prediction = y_test
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

    #Leaving one out
    print("\nleaving one out")
    loo=LeaveOneOut()
    for train_index, test_index in loo.split(fruit):
        X_train, X_test = glass_x[train_index], glass_x[test_index]
        y_train, y_test = glass_y[train_index], glass_y[test_index]
    clf = KNN(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    for i in prediction:
        print(i, end= ' ')

    prediction == y_test
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

Output : 
###########################################################################

k-value= 3

90-10
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 6 1 1 7 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
90.9090909090909

70-30

 
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 6 1 1 7 2 7 7 7 3 1 1 1 5 1 1 2 3 2 1 7 5 3 2 2 2 7 1 2 3 2 1 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
93.84615384615384

leaving one out
1 [1]
accuracy: 
100.0
###########################################################################

k-value= 5

90-10
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 6 1 1 7 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
90.9090909090909

70-30

 
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 6 1 1 7 2 7 7 7 3 1 1 1 5 1 1 2 3 2 1 7 5 3 2 2 2 7 1 2 3 2 1 2 1 2 1 1 1 2 1 1 7 2 6 1 1 2 1 7 5 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
90.76923076923077

leaving one out
1 [1]
accuracy: 
100.0
###########################################################################

k-value= 7

90-10
1 7 1 7 2 2 1 2 2 2 5 5 2 2 6 5 7 1 1 7 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
90.9090909090909

70-30

 
1 7 1 7 2 2 1 2 2 2 7 5 2 2 6 5 7 1 1 7 2 7 7 7 3 1 1 1 5 1 1 2 3 2 1 7 7 3 2 2 2 7 1 2 3 2 1 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
92.3076923076923

leaving one out
1 [1]
accuracy: 
100.0


#Manhattan
class KNNClassifier:
    def __init__(self, k):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X_test):
        y_pred = []
        for x in X_test:
            distances = [cityblock(x, x_train) for x_train in self.X_train]
            sorted_indices = np.argsort(distances)[:self.k]
            k_nearest_labels = [self.y_train[i] for i in sorted_indices]
            predicted_label = max(set(k_nearest_labels), key=k_nearest_labels.count)
            y_pred.append(predicted_label)
        return y_pred
    
    def score(self, X_test, y_test):
        predictions = self.predict(X_test)
        return (predictions == y_test).sum() / len(y_test)

kvalues=[3,5,7]
for k in kvalues:
    print("###########################################################################")
    print(f"\nk-value= {k}")
    #splitting the data in 90-10
    print("\n90-10")
    X_train, X_test, y_train, y_test = train_test_split(glass_x, glass_y, random_state = 42, test_size = 0.1)
    clf = KNNClassifier(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    for i in prediction:
        print(i, end= ' ')

    prediction = y_test
    print("\n")
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

    #splitting the data to 70-30
    print("\n70-30")
    print("\n ")
    X_train, X_test, y_train, y_test = train_test_split(glass_x, glass_y, random_state = 42, test_size = 0.3)
    clf = KNNClassifier(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
 
    for i in prediction:
        print(i, end= ' ')

    prediction = y_test
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

    #Leaving one out
    print("\nleaving one out")
    loo=LeaveOneOut()
    for train_index, test_index in loo.split(glass):
        X_train, X_test = glass_x[train_index], glass_x[test_index]
        y_train, y_test = glass_y[train_index], glass_y[test_index]
    clf = KNNClassifier(k)
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    for i in prediction:
        print(i, end= ' ')

    prediction == y_test
    print(prediction)
    print("accuracy: ")
    print(clf.score(X_test, y_test)*100)

###########################################################################

k-value= 3

90-10
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
100.0

70-30

 
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
100.0

leaving one out
7 [7]
accuracy: 
100.0
###########################################################################

k-value= 5

90-10
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 7 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
95.45454545454545

70-30

 
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 7 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
98.46153846153847

leaving one out
7 [7]
accuracy: 
100.0
###########################################################################

k-value= 7

90-10
1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 7 2 7 

[1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7]
accuracy: 
95.45454545454545

70-30

 
1 7 1 7 2 2 1 2 2 2 6 2 2 2 6 5 7 1 1 7 2 7 7 7 3 1 1 1 5 1 1 2 3 2 1 7 5 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 6 2 5 1 1 2 1 7 2 [1 7 1 7 2 2 1 2 2 2 6 5 2 2 6 5 7 1 1 6 2 7 7 7 3 2 1 1 5 1 1 2 3 2 1 7 5
 3 2 2 2 7 1 2 3 2 2 2 2 2 1 1 1 2 1 1 7 2 5 1 1 2 1 7 5]
accuracy: 
92.3076923076923

leaving one out
7 [7]
accuracy: 
100.0